{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#!pip install fastapi==0.103.1 uvicorn==0.23.2 pydantic==2.3.0 python-multipart==0.0.6 jinja2==3.1.2 starlette==0.27.0 langchain-core==0.1.0 langchain-mistralai==0.0.3 langgraph==0.0.17 tavily-python==0.2.6 python-dotenv==1.0.0 requests==2.31.0 aiohttp==3.8.5"
      ],
      "metadata": {
        "id": "oz63NwsE0vGM"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastapi uvicorn pydantic jinja2 python-multipart\n",
        "!pip install langchain-core langchain-mistralai langgraph\n",
        "!pip install tavily-python\n",
        "!pip install python-dotenv requests aiohttp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Res3x5Re0_r7",
        "outputId": "dcf614fc-c3a0-4b22-d6a4-e70b094c2c42"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.11/dist-packages (0.115.12)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.11/dist-packages (0.34.2)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (2.11.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (3.1.6)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.11/dist-packages (0.0.20)\n",
            "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (0.46.2)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (4.13.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.1.8)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic) (0.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2) (3.0.2)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi) (4.9.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (1.3.1)\n",
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.11/dist-packages (0.3.55)\n",
            "Requirement already satisfied: langchain-mistralai in /usr/local/lib/python3.11/dist-packages (0.2.10)\n",
            "Requirement already satisfied: langgraph in /usr/local/lib/python3.11/dist-packages (0.3.34)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (0.3.33)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (4.13.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (2.11.3)\n",
            "Requirement already satisfied: tokenizers<1,>=0.15.1 in /usr/local/lib/python3.11/dist-packages (from langchain-mistralai) (0.21.1)\n",
            "Requirement already satisfied: httpx<1,>=0.25.2 in /usr/local/lib/python3.11/dist-packages (from langchain-mistralai) (0.28.1)\n",
            "Requirement already satisfied: httpx-sse<1,>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from langchain-mistralai) (0.4.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.0.10 in /usr/local/lib/python3.11/dist-packages (from langgraph) (2.0.24)\n",
            "Requirement already satisfied: langgraph-prebuilt<0.2,>=0.1.8 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.1.8)\n",
            "Requirement already satisfied: langgraph-sdk<0.2.0,>=0.1.42 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.1.63)\n",
            "Requirement already satisfied: xxhash<4.0.0,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from langgraph) (3.5.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.2->langchain-mistralai) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.2->langchain-mistralai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.2->langchain-mistralai) (1.0.8)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.2->langchain-mistralai) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.25.2->langchain-mistralai) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\n",
            "Requirement already satisfied: ormsgpack<2.0.0,>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph) (1.9.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10.16)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core) (0.4.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers<1,>=0.15.1->langchain-mistralai) (0.30.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15.1->langchain-mistralai) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15.1->langchain-mistralai) (2025.3.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15.1->langchain-mistralai) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core) (2.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.25.2->langchain-mistralai) (1.3.1)\n",
            "Requirement already satisfied: tavily-python in /usr/local/lib/python3.11/dist-packages (0.7.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from tavily-python) (2.32.3)\n",
            "Requirement already satisfied: tiktoken>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from tavily-python) (0.9.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from tavily-python) (0.28.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken>=0.5.1->tavily-python) (2024.11.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->tavily-python) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->tavily-python) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->tavily-python) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->tavily-python) (2025.1.31)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->tavily-python) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->tavily-python) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->tavily-python) (0.14.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx->tavily-python) (1.3.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx->tavily-python) (4.13.2)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (1.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (3.11.15)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (1.20.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import json\n",
        "import logging\n",
        "import hashlib\n",
        "import datetime\n",
        "from typing import List, Dict, Any, Optional\n",
        "from pathlib import Path\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "# FastAPI imports\n",
        "from fastapi import FastAPI, Request, HTTPException, Depends, Cookie\n",
        "from fastapi.responses import HTMLResponse, JSONResponse\n",
        "from fastapi.staticfiles import StaticFiles\n",
        "from fastapi.templating import Jinja2Templates\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from starlette.responses import RedirectResponse\n",
        "\n",
        "# Import research agent components\n",
        "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
        "from langchain_mistralai import ChatMistralAI\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[logging.StreamHandler()]\n",
        ")\n",
        "logger = logging.getLogger('research_agent_api')\n",
        "\n",
        "# Configuration class for the research agent\n",
        "class AgentConfig:\n",
        "    def __init__(\n",
        "        self,\n",
        "        tavily_api_key: str = None,\n",
        "        mistral_api_key: str = None,\n",
        "        mistral_model: str = \"mistral-small\",\n",
        "        max_search_results: int = 5,\n",
        "        temperature: float = 0.1,\n",
        "        search_depth: str = \"advanced\",\n",
        "        enable_cache: bool = True,\n",
        "        cache_dir: str = \"./research_cache\",\n",
        "        cache_ttl: int = 86400,  # 24 hours in seconds\n",
        "        max_retries: int = 3,\n",
        "        backoff_factor: float = 2.0,\n",
        "        verbose: bool = True,\n",
        "    ):\n",
        "        self.tavily_api_key = tavily_api_key\n",
        "        self.mistral_api_key = mistral_api_key\n",
        "        self.mistral_model = mistral_model\n",
        "        self.max_search_results = max_search_results\n",
        "        self.temperature = temperature\n",
        "        self.search_depth = search_depth\n",
        "        self.enable_cache = enable_cache\n",
        "        self.cache_dir = Path(cache_dir)\n",
        "        self.cache_ttl = cache_ttl\n",
        "        self.max_retries = max_retries\n",
        "        self.backoff_factor = backoff_factor\n",
        "        self.verbose = verbose\n",
        "\n",
        "        # Create cache directory if it doesn't exist\n",
        "        if self.enable_cache:\n",
        "            os.makedirs(self.cache_dir, exist_ok=True)\n",
        "\n",
        "    @classmethod\n",
        "    def from_env(cls, **kwargs):\n",
        "        \"\"\"Create a configuration from environment variables\"\"\"\n",
        "        config = cls(**kwargs)\n",
        "\n",
        "        # Override with environment variables if they exist\n",
        "        if os.environ.get(\"TAVILY_API_KEY\"):\n",
        "            config.tavily_api_key = os.environ.get(\"TAVILY_API_KEY\")\n",
        "        if os.environ.get(\"MISTRAL_API_KEY\"):\n",
        "            config.mistral_api_key = os.environ.get(\"MISTRAL_API_KEY\")\n",
        "        if os.environ.get(\"MISTRAL_MODEL\"):\n",
        "            config.mistral_model = os.environ.get(\"MISTRAL_MODEL\")\n",
        "\n",
        "        return config\n",
        "\n",
        "    def to_dict(self):\n",
        "        \"\"\"Convert config to dictionary for serialization\"\"\"\n",
        "        return {\n",
        "            \"mistral_model\": self.mistral_model,\n",
        "            \"max_search_results\": self.max_search_results,\n",
        "            \"temperature\": self.temperature,\n",
        "            \"search_depth\": self.search_depth,\n",
        "            \"enable_cache\": self.enable_cache,\n",
        "            \"cache_ttl\": self.cache_ttl,\n",
        "            \"max_retries\": self.max_retries,\n",
        "            \"verbose\": self.verbose\n",
        "        }\n",
        "\n",
        "# Simple caching mechanism\n",
        "class ResultCache:\n",
        "    \"\"\"Simple file-based cache for research results\"\"\"\n",
        "\n",
        "    def __init__(self, config: AgentConfig):\n",
        "        self.config = config\n",
        "        self.cache_dir = config.cache_dir\n",
        "        # Create cache directory if it doesn't exist\n",
        "        os.makedirs(self.cache_dir, exist_ok=True)\n",
        "\n",
        "    def _generate_key(self, query: str) -> str:\n",
        "        \"\"\"Generate a cache key from a query\"\"\"\n",
        "        # Create a deterministic hash of the query\n",
        "        return hashlib.md5(query.encode()).hexdigest()\n",
        "\n",
        "    def _get_cache_path(self, key: str) -> Path:\n",
        "        \"\"\"Get the file path for a cache key\"\"\"\n",
        "        return self.cache_dir / f\"{key}.json\"\n",
        "\n",
        "    def get(self, query: str) -> Optional[Dict[str, Any]]:\n",
        "        \"\"\"Get cached results for a query if they exist and are fresh\"\"\"\n",
        "        if not self.config.enable_cache:\n",
        "            return None\n",
        "\n",
        "        key = self._generate_key(query)\n",
        "        cache_path = self._get_cache_path(key)\n",
        "\n",
        "        if not cache_path.exists():\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            # Check if cache is expired\n",
        "            if time.time() - cache_path.stat().st_mtime > self.config.cache_ttl:\n",
        "                logger.info(f\"Cache expired for query: {query}\")\n",
        "                return None\n",
        "\n",
        "            # Load cached data\n",
        "            with open(cache_path, 'r') as f:\n",
        "                data = json.load(f)\n",
        "\n",
        "            logger.info(f\"Cache hit for query: {query}\")\n",
        "            return data\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Error reading cache: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def set(self, query: str, data: Dict[str, Any]) -> None:\n",
        "        \"\"\"Cache results for a query\"\"\"\n",
        "        if not self.config.enable_cache:\n",
        "            return\n",
        "\n",
        "        key = self._generate_key(query)\n",
        "        cache_path = self._get_cache_path(key)\n",
        "\n",
        "        try:\n",
        "            with open(cache_path, 'w') as f:\n",
        "                json.dump(data, f)\n",
        "            logger.info(f\"Cached results for query: {query}\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Error writing cache: {str(e)}\")\n",
        "\n",
        "# Create a direct Tavily search function with caching\n",
        "def create_search_function(config: AgentConfig):\n",
        "    \"\"\"Create a function that searches using Tavily API directly with caching\"\"\"\n",
        "    from tavily import TavilyClient\n",
        "\n",
        "    # Initialize cache\n",
        "    cache = ResultCache(config)\n",
        "\n",
        "    # Explicitly pass the API key to the client\n",
        "    tavily_client = TavilyClient(api_key=config.tavily_api_key)\n",
        "\n",
        "    def search(query: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Search the web using Tavily API with caching and retries\"\"\"\n",
        "        try:\n",
        "            # Check cache first\n",
        "            cached_results = cache.get(query)\n",
        "            if cached_results:\n",
        "                logger.info(f\"Using cached results for: {query}\")\n",
        "                return cached_results\n",
        "\n",
        "            logger.info(f\"Searching for: {query}\")\n",
        "\n",
        "            # Try to search with retries and exponential backoff\n",
        "            max_retries = config.max_retries\n",
        "            backoff = 1\n",
        "\n",
        "            for attempt in range(max_retries):\n",
        "                try:\n",
        "                    response = tavily_client.search(\n",
        "                        query=query,\n",
        "                        search_depth=config.search_depth,\n",
        "                        max_results=config.max_search_results\n",
        "                    )\n",
        "                    results = response.get(\"results\", [])\n",
        "\n",
        "                    # Cache the results\n",
        "                    cache.set(query, results)\n",
        "\n",
        "                    return results\n",
        "\n",
        "                except Exception as e:\n",
        "                    if attempt < max_retries - 1:\n",
        "                        wait_time = backoff * config.backoff_factor\n",
        "                        logger.warning(f\"Search error, retrying in {wait_time}s: {str(e)}\")\n",
        "                        time.sleep(wait_time)\n",
        "                        backoff *= config.backoff_factor\n",
        "                    else:\n",
        "                        raise\n",
        "\n",
        "            # Fallback to basic search if we get here\n",
        "            logger.info(\"Falling back to basic search...\")\n",
        "\n",
        "            response = tavily_client.search(\n",
        "                query=query,\n",
        "                search_depth=\"basic\",\n",
        "                max_results=config.max_search_results\n",
        "            )\n",
        "            results = response.get(\"results\", [])\n",
        "\n",
        "            # Cache the results\n",
        "            cache.set(query, results)\n",
        "\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Search failed after all retry attempts: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "    return search\n",
        "\n",
        "# Initialize the LLM\n",
        "def create_llm(config: AgentConfig):\n",
        "    \"\"\"Create and configure the LLM\"\"\"\n",
        "    try:\n",
        "        llm = ChatMistralAI(\n",
        "            model=config.mistral_model,\n",
        "            temperature=config.temperature,\n",
        "            max_tokens=4000,  # Allow for longer responses\n",
        "            api_key=config.mistral_api_key  # Explicitly pass the API key\n",
        "        )\n",
        "        return llm\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error creating LLM: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Define the state schema for our agent system\n",
        "class AgentState(dict):\n",
        "    \"\"\"Dictionary subclass for agent state with typed annotations for clarity\"\"\"\n",
        "    query: str\n",
        "    refined_query: Optional[str]\n",
        "    research_results: List[Dict[str, Any]]\n",
        "    research_summary: str\n",
        "    draft_answer: str\n",
        "    final_answer: str\n",
        "    messages: List[Any]  # List of message objects\n",
        "    start_time: float\n",
        "    metadata: Dict[str, Any]\n",
        "    error_count: int\n",
        "\n",
        "# Query refinement node\n",
        "def refine_query(state: AgentState, config: AgentConfig) -> AgentState:\n",
        "    \"\"\"Refine the user's query to make it more effective for research\"\"\"\n",
        "    logger.info(\"Refining query: Optimizing for better search results\")\n",
        "\n",
        "    try:\n",
        "        # Extract query from state\n",
        "        query = state[\"query\"]\n",
        "\n",
        "        # Create LLM\n",
        "        llm = create_llm(config)\n",
        "\n",
        "        # Create messages for query refinement\n",
        "        messages = [\n",
        "            SystemMessage(content=\"\"\"You are a query optimization expert. Your task is to refine search queries to get better search results.\n",
        "            Make the query more specific, clear, and effective for web searching. Do not change the original intent of the query.\n",
        "            If the query is already well-formulated, you can return it as is.\"\"\"),\n",
        "            HumanMessage(content=f\"\"\"\n",
        "            Original Query: {query}\n",
        "\n",
        "            Please refine this query to make it more effective for web searching. Focus on:\n",
        "            1. Making it more specific if needed\n",
        "            2. Adding relevant keywords\n",
        "            3. Removing unnecessary words\n",
        "            4. Making it clearer and more precise\n",
        "\n",
        "            Return ONLY the refined query, with no additional explanation or commentary.\n",
        "            \"\"\")\n",
        "        ]\n",
        "\n",
        "        # Get refined query\n",
        "        logger.info(\"Generating refined query...\")\n",
        "\n",
        "        refined_response = llm.invoke(messages)\n",
        "        refined_query = refined_response.content.strip()\n",
        "\n",
        "        # Check if query was actually refined\n",
        "        if refined_query.lower() == query.lower():\n",
        "            refined_query = query\n",
        "            logger.info(\"Query is already well-formulated\")\n",
        "        else:\n",
        "            logger.info(f\"Query refined: {refined_query}\")\n",
        "\n",
        "        # Update state\n",
        "        state[\"refined_query\"] = refined_query\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in query refinement: {str(e)}\")\n",
        "        # Fall back to original query\n",
        "        state[\"refined_query\"] = state[\"query\"]\n",
        "        # Increment error count\n",
        "        state[\"error_count\"] += 1\n",
        "\n",
        "    return state\n",
        "\n",
        "# Research node implementation with progress tracking\n",
        "def research(state: AgentState, config: AgentConfig) -> AgentState:\n",
        "    \"\"\"Research node: Search for information and summarize findings\"\"\"\n",
        "    # Extract query from state\n",
        "    query = state[\"query\"]\n",
        "    refined_query = state.get(\"refined_query\", query)\n",
        "    search_query = refined_query if refined_query else query\n",
        "\n",
        "    logger.info(f\"RESEARCH: Searching for information on '{search_query}'\")\n",
        "\n",
        "    try:\n",
        "        # Get search function and LLM\n",
        "        search_function = create_search_function(config)\n",
        "        llm = create_llm(config)\n",
        "\n",
        "        # Execute search\n",
        "        search_results = search_function(search_query)\n",
        "\n",
        "        if not search_results:\n",
        "            logger.warning(\"No search results found\")\n",
        "            research_summary = \"The search did not return any results. Please try a different query or check your internet connection.\"\n",
        "            state[\"research_results\"] = []\n",
        "            state[\"research_summary\"] = research_summary\n",
        "            state[\"messages\"] = [\n",
        "                HumanMessage(content=query),\n",
        "                AIMessage(content=research_summary)\n",
        "            ]\n",
        "            state[\"error_count\"] += 1\n",
        "            return state\n",
        "\n",
        "        # Format search results for the LLM\n",
        "        results_text = \"\"\n",
        "        for i, result in enumerate(search_results):\n",
        "            results_text += f\"Source {i+1}: {result.get('title', 'No title')}\\n\"\n",
        "            results_text += f\"URL: {result.get('url', 'No URL')}\\n\"\n",
        "            results_text += f\"Content: {result.get('content', 'No content')}\\n\\n\"\n",
        "\n",
        "        # Create messages for the LLM\n",
        "        messages = [\n",
        "            SystemMessage(content=\"\"\"You are a research assistant. Based on the search results provided, create a comprehensive summary.\n",
        "            Focus on synthesizing information from multiple sources, identifying key facts, insights, and different perspectives.\n",
        "            Be objective and thorough in your summary. Include relevant statistics, dates, and findings from the search results.\n",
        "            Identify any conflicting information or gaps in the available data.\"\"\"),\n",
        "            HumanMessage(content=f\"\"\"\n",
        "            Search Query: {search_query}\n",
        "\n",
        "            Search Results:\n",
        "            {results_text}\n",
        "\n",
        "            Please provide a detailed summary of the information found in these search results.\n",
        "            Focus on key facts, insights, and different perspectives on the topic.\n",
        "            Highlight any consensus views as well as areas of debate or uncertainty.\n",
        "            \"\"\")\n",
        "        ]\n",
        "\n",
        "        # Get research summary\n",
        "        logger.info(\"Generating research summary...\")\n",
        "        summary_response = llm.invoke(messages)\n",
        "        research_summary = summary_response.content\n",
        "\n",
        "        logger.info(\"Research summary generated\")\n",
        "\n",
        "        # Update state\n",
        "        state[\"research_results\"] = search_results\n",
        "        state[\"research_summary\"] = research_summary\n",
        "        state[\"messages\"] = [\n",
        "            HumanMessage(content=query),\n",
        "            summary_response\n",
        "        ]\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in research node: {str(e)}\")\n",
        "        # Provide graceful failure\n",
        "        state[\"research_results\"] = []\n",
        "        state[\"research_summary\"] = f\"Research failed due to an error: {str(e)}. Please try again later.\"\n",
        "        state[\"messages\"] = [\n",
        "            HumanMessage(content=query),\n",
        "            AIMessage(content=f\"I encountered an error while researching: {str(e)}. Please try again or modify your query.\")\n",
        "        ]\n",
        "        # Increment error count\n",
        "        state[\"error_count\"] += 1\n",
        "\n",
        "    return state\n",
        "\n",
        "# Draft answer node implementation\n",
        "def draft_answer(state: AgentState, config: AgentConfig) -> AgentState:\n",
        "    \"\"\"Draft answer node: Create initial answer based on research\"\"\"\n",
        "    logger.info(\"DRAFTING: Creating initial answer based on research\")\n",
        "\n",
        "    try:\n",
        "        # Extract required information\n",
        "        query = state[\"query\"]\n",
        "        research_summary = state[\"research_summary\"]\n",
        "\n",
        "        # Handle case where research failed\n",
        "        if not research_summary or research_summary.startswith(\"Research failed\"):\n",
        "            logger.warning(\"Research summary not available, creating limited draft\")\n",
        "            state[\"draft_answer\"] = \"Unable to provide a comprehensive answer due to research limitations.\"\n",
        "            state[\"error_count\"] += 1\n",
        "            return state\n",
        "\n",
        "        # Get LLM\n",
        "        llm = create_llm(config)\n",
        "\n",
        "        # Create messages for drafting\n",
        "        messages = [\n",
        "            SystemMessage(content=\"\"\"You are an expert content writer. Your task is to draft a comprehensive answer based on the research summary provided.\n",
        "            Focus on accuracy, clarity, and logical organization of information. Present different perspectives where appropriate.\n",
        "            Use paragraphs, bullet points, and other formatting as needed to make the content readable and engaging.\n",
        "            Cite specific facts, statistics, and findings from the research summary. Don't add information not found in the research.\"\"\"),\n",
        "            HumanMessage(content=f\"\"\"\n",
        "            Original Query: {query}\n",
        "\n",
        "            Research Summary:\n",
        "            {research_summary}\n",
        "\n",
        "            Create a well-structured draft answer based on this research. Focus on clarity, accuracy, and coherence.\n",
        "            Organize the information logically and make it engaging. Include all relevant facts and insights from the research.\n",
        "            \"\"\")\n",
        "        ]\n",
        "\n",
        "        # Get draft answer\n",
        "        logger.info(\"Generating draft answer...\")\n",
        "        draft_response = llm.invoke(messages)\n",
        "        draft_answer = draft_response.content\n",
        "\n",
        "        logger.info(\"Draft answer generated\")\n",
        "\n",
        "        # Update state\n",
        "        state[\"draft_answer\"] = draft_answer\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in draft node: {str(e)}\")\n",
        "        # Provide graceful failure\n",
        "        state[\"draft_answer\"] = f\"Unable to draft an answer due to an error: {str(e)}. Please try again later.\"\n",
        "        # Increment error count\n",
        "        state[\"error_count\"] += 1\n",
        "\n",
        "    return state\n",
        "\n",
        "# Finalize answer node implementation\n",
        "def finalize_answer(state: AgentState, config: AgentConfig) -> AgentState:\n",
        "    \"\"\"Finalize answer node: Polish and improve the draft\"\"\"\n",
        "    logger.info(\"FINALIZING: Polishing and improving the answer\")\n",
        "\n",
        "    try:\n",
        "        # Extract required information\n",
        "        query = state[\"query\"]\n",
        "        research_summary = state[\"research_summary\"]\n",
        "        draft_answer = state[\"draft_answer\"]\n",
        "\n",
        "        # Handle case where drafting failed\n",
        "        if not draft_answer or draft_answer.startswith(\"Unable to draft\"):\n",
        "            logger.warning(\"Draft not available, providing limited final answer\")\n",
        "            state[\"final_answer\"] = \"I couldn't complete the research process successfully. Please try again with a different query.\"\n",
        "            state[\"error_count\"] += 1\n",
        "            return state\n",
        "\n",
        "        # Get LLM\n",
        "        llm = create_llm(config)\n",
        "\n",
        "        # Create messages for finalizing\n",
        "        messages = [\n",
        "            SystemMessage(content=\"\"\"You are an expert editor and reviewer. Your task is to review the draft answer and improve it into a final version.\n",
        "            Focus on enhancing clarity, readability, and coherence while maintaining accuracy.\n",
        "            Improve the structure, flow, and transitions between sections.\n",
        "            Ensure consistent tone, eliminate redundancies, and correct any grammatical or stylistic issues.\n",
        "            Make the answer more engaging and accessible without compromising on substance.\"\"\"),\n",
        "            HumanMessage(content=f\"\"\"\n",
        "            Original Query: {query}\n",
        "\n",
        "            Research Summary:\n",
        "            {research_summary}\n",
        "\n",
        "            Draft Answer:\n",
        "            {draft_answer}\n",
        "\n",
        "            Please review the draft and provide a polished final answer. Ensure it is:\n",
        "            1. Accurate - correctly represents the research findings\n",
        "            2. Comprehensive - includes all important information\n",
        "            3. Well-structured - follows a logical flow\n",
        "            4. Clear and engaging - easy to understand\n",
        "            5. Free of errors - grammatically correct and properly formatted\n",
        "\n",
        "            Provide only the final answer without any meta-commentary.\n",
        "            \"\"\")\n",
        "        ]\n",
        "\n",
        "        # Get final answer\n",
        "        logger.info(\"Generating final answer...\")\n",
        "        final_response = llm.invoke(messages)\n",
        "        final_answer = final_response.content\n",
        "\n",
        "        logger.info(\"Final answer generated\")\n",
        "\n",
        "        # Update state\n",
        "        state[\"final_answer\"] = final_answer\n",
        "\n",
        "        # Update metadata with timing information\n",
        "        elapsed_time = time.time() - state[\"start_time\"]\n",
        "        state[\"metadata\"][\"total_time\"] = elapsed_time\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in finalize node: {str(e)}\")\n",
        "        # Provide graceful failure\n",
        "        state[\"final_answer\"] = f\"I was unable to finalize the answer due to an error: {str(e)}. Here is the draft answer instead:\\n\\n{state['draft_answer']}\"\n",
        "        # Increment error count\n",
        "        state[\"error_count\"] += 1\n",
        "\n",
        "    return state\n",
        "\n",
        "# Create the graph with configuration\n",
        "def create_research_workflow(config: AgentConfig):\n",
        "    \"\"\"Create and compile the agent workflow graph\"\"\"\n",
        "    logger.info(\"Creating research workflow graph...\")\n",
        "\n",
        "    try:\n",
        "        # Initialize the graph\n",
        "        workflow = StateGraph(AgentState)\n",
        "\n",
        "        # Add nodes with configuration as argument\n",
        "        workflow.add_node(\"refine_query\", lambda state: refine_query(state, config))\n",
        "        workflow.add_node(\"research\", lambda state: research(state, config))\n",
        "        workflow.add_node(\"draft\", lambda state: draft_answer(state, config))\n",
        "        workflow.add_node(\"finalize\", lambda state: finalize_answer(state, config))\n",
        "\n",
        "        # Add edges with the new query refinement step\n",
        "        workflow.add_edge(\"refine_query\", \"research\")\n",
        "        workflow.add_edge(\"research\", \"draft\")\n",
        "        workflow.add_edge(\"draft\", \"finalize\")\n",
        "        workflow.add_edge(\"finalize\", END)\n",
        "\n",
        "        # Set the entry point\n",
        "        workflow.set_entry_point(\"refine_query\")\n",
        "\n",
        "        # Compile the graph\n",
        "        compiled_workflow = workflow.compile()\n",
        "\n",
        "        logger.info(\"Workflow graph created and compiled successfully\")\n",
        "\n",
        "        return compiled_workflow\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error creating workflow graph: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Function to run the research system with improved error handling\n",
        "def run_research_system(query: str, config: AgentConfig):\n",
        "    \"\"\"Run the full research workflow\"\"\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    logger.info(f\"STARTING RESEARCH WORKFLOW: '{query}'\")\n",
        "\n",
        "    try:\n",
        "        # Input validation\n",
        "        if not query or not query.strip():\n",
        "            logger.error(\"Query cannot be empty\")\n",
        "            return {\n",
        "                \"query\": \"\",\n",
        "                \"refined_query\": \"\",\n",
        "                \"research_results\": [],\n",
        "                \"research_summary\": \"No query provided\",\n",
        "                \"draft_answer\": \"\",\n",
        "                \"final_answer\": \"Please provide a valid search query\",\n",
        "                \"messages\": [],\n",
        "                \"start_time\": start_time,\n",
        "                \"metadata\": {\"error\": \"Empty query\"},\n",
        "                \"error_count\": 1\n",
        "            }\n",
        "\n",
        "        # Create the workflow with configuration\n",
        "        research_workflow = create_research_workflow(config)\n",
        "\n",
        "        # Initialize the state\n",
        "        initial_state = {\n",
        "            \"query\": query,\n",
        "            \"refined_query\": None,\n",
        "            \"research_results\": [],\n",
        "            \"research_summary\": \"\",\n",
        "            \"draft_answer\": \"\",\n",
        "            \"final_answer\": \"\",\n",
        "            \"messages\": [],\n",
        "            \"start_time\": start_time,\n",
        "            \"metadata\": {\"config\": config.to_dict()},\n",
        "            \"error_count\": 0\n",
        "        }\n",
        "\n",
        "        # Execute the workflow\n",
        "        result = research_workflow.invoke(initial_state)\n",
        "\n",
        "        # Calculate execution time\n",
        "        execution_time = time.time() - start_time\n",
        "\n",
        "        logger.info(f\"Research completed in {execution_time:.2f} seconds!\")\n",
        "\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in research workflow: {str(e)}\")\n",
        "        # Return partial results\n",
        "        return {\n",
        "            \"query\": query,\n",
        "            \"refined_query\": None,\n",
        "            \"research_results\": [],\n",
        "            \"research_summary\": f\"Research workflow failed: {str(e)}\",\n",
        "            \"draft_answer\": \"\",\n",
        "            \"final_answer\": f\"I encountered an error while processing your query: {str(e)}. Please try again later.\",\n",
        "            \"messages\": [],\n",
        "            \"start_time\": start_time,\n",
        "            \"metadata\": {\"error\": str(e)},\n",
        "            \"error_count\": 1\n",
        "        }\n",
        "\n",
        "# Pydantic models for API requests and responses\n",
        "class SetupRequest(BaseModel):\n",
        "    tavily_api_key: str\n",
        "    mistral_api_key: str\n",
        "    model: str = \"mistral-small\"\n",
        "    max_results: int = 5\n",
        "    temperature: float = 0.1\n",
        "    search_depth: str = \"advanced\"\n",
        "    enable_cache: bool = True\n",
        "\n",
        "class ResearchRequest(BaseModel):\n",
        "    query: str\n",
        "    model: Optional[str] = None\n",
        "    max_results: Optional[int] = None\n",
        "    temperature: Optional[float] = None\n",
        "    search_depth: Optional[str] = None\n",
        "    enable_cache: Optional[bool] = None\n",
        "\n",
        "class ResearchResponse(BaseModel):\n",
        "    query: str\n",
        "    refined_query: str = \"\"\n",
        "    research_summary: str = \"\"\n",
        "    draft_answer: str = \"\"\n",
        "    final_answer: str = \"\"\n",
        "    research_results: List[Dict[str, Any]] = []\n",
        "    timestamp: str = \"\"\n",
        "    metadata: Dict[str, Any] = {}\n",
        "\n",
        "class ConfigResponse(BaseModel):\n",
        "    mistral_model: str\n",
        "    max_search_results: int\n",
        "    temperature: float\n",
        "    search_depth: str\n",
        "    enable_cache: bool\n",
        "    cache_ttl: int = 86400\n",
        "    max_retries: int = 3\n",
        "    verbose: bool = True\n",
        "\n",
        "# Create FastAPI app\n",
        "app = FastAPI(title=\"Research Agent API\")\n",
        "\n",
        "# Add CORS middleware\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "# Create a simple in-memory store for session data\n",
        "# In a production app, you'd use a database or Redis\n",
        "session_store = {}\n",
        "\n",
        "# Create templates directory\n",
        "templates = Jinja2Templates(directory=\"templates\")\n",
        "\n",
        "# Create templates directory\n",
        "os.makedirs(\"templates\", exist_ok=True)\n",
        "\n",
        "# Write the index.html template\n",
        "with open(\"templates/index.html\", \"w\") as f:\n",
        "    f.write(\"\"\"<!DOCTYPE html>\n",
        "<html lang=\"en\">\n",
        "<head>\n",
        "    <meta charset=\"UTF-8\">\n",
        "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
        "    <title>Research Agent</title>\n",
        "    <link href=\"https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/css/bootstrap.min.css\" rel=\"stylesheet\">\n",
        "    <link rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/bootstrap-icons@1.10.0/font/bootstrap-icons.css\">\n",
        "    <style>\n",
        "        body {\n",
        "            padding-top: 20px;\n",
        "            background-color: #f8f9fa;\n",
        "        }\n",
        "        .research-card {\n",
        "            transition: all 0.3s;\n",
        "        }\n",
        "        .research-card:hover {\n",
        "            transform: translateY(-5px);\n",
        "            box-shadow: 0 10px 20px rgba(0,0,0,0.1);\n",
        "        }\n",
        "        .loading {\n",
        "            display: none;\n",
        "        }\n",
        "        .api-key-section {\n",
        "            background-color: #f0f8ff;\n",
        "            border-radius: 10px;\n",
        "            padding: 20px;\n",
        "            margin-bottom: 20px;\n",
        "        }\n",
        "        .history-item {\n",
        "            cursor: pointer;\n",
        "        }\n",
        "        .history-item:hover {\n",
        "            background-color: #f0f8ff;\n",
        "        }\n",
        "        #searchResults {\n",
        "            max-height: 300px;\n",
        "            overflow-y: auto;\n",
        "        }\n",
        "        .source-url {\n",
        "            word-break: break-all;\n",
        "        }\n",
        "        .tab-content {\n",
        "            padding: 20px;\n",
        "            background-color: #fff;\n",
        "            border: 1px solid #dee2e6;\n",
        "            border-top: none;\n",
        "            border-radius: 0 0 5px 5px;\n",
        "        }\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "    <div class=\"container\">\n",
        "        <h1 class=\"text-center mb-4\">Research Assistant</h1>\n",
        "\n",
        "        <!-- API Key Setup -->\n",
        "        <div class=\"api-key-section\" id=\"apiKeySection\">\n",
        "            <h3>Setup API Keys</h3>\n",
        "            <div class=\"mb-3\">\n",
        "                <label for=\"tavilyApiKey\" class=\"form-label\">Tavily API Key</label>\n",
        "                <input type=\"password\" class=\"form-control\" id=\"tavilyApiKey\" placeholder=\"Enter Tavily API Key\">\n",
        "            </div>\n",
        "            <div class=\"mb-3\">\n",
        "                <label for=\"mistralApiKey\" class=\"form-label\">Mistral API Key</label>\n",
        "                <input type=\"password\" class=\"form-control\" id=\"mistralApiKey\" placeholder=\"Enter Mistral API Key\">\n",
        "            </div>\n",
        "            <div class=\"mb-3\">\n",
        "                <label for=\"modelSelect\" class=\"form-label\">Model</label>\n",
        "                <select class=\"form-select\" id=\"modelSelect\">\n",
        "                    <option value=\"mistral-small\">Mistral Small</option>\n",
        "                    <option value=\"mistral-medium\">Mistral Medium</option>\n",
        "                    <option value=\"mistral-large\">Mistral Large</option>\n",
        "                </select>\n",
        "            </div>\n",
        "            <div class=\"row\">\n",
        "                <div class=\"col-md-6\">\n",
        "                    <div class=\"mb-3\">\n",
        "                        <label for=\"maxResults\" class=\"form-label\">Max Search Results</label>\n",
        "                        <input type=\"number\" class=\"form-control\" id=\"maxResults\" value=\"5\" min=\"1\" max=\"10\">\n",
        "                    </div>\n",
        "                </div>\n",
        "                <div class=\"col-md-6\">\n",
        "                    <div class=\"mb-3\">\n",
        "                        <label for=\"temperature\" class=\"form-label\">Temperature</label>\n",
        "                        <input type=\"range\" class=\"form-range\" id=\"temperature\" min=\"0\" max=\"1\" step=\"0.1\" value=\"0.1\">\n",
        "                        <span id=\"temperatureValue\">0.1</span>\n",
        "                    </div>\n",
        "                </div>\n",
        "            </div>\n",
        "            <div class=\"mb-3\">\n",
        "                <label for=\"searchDepth\" class=\"form-label\">Search Depth</label>\n",
        "                <select class=\"form-select\" id=\"searchDepth\">\n",
        "                    <option value=\"basic\">Basic</option>\n",
        "                    <option value=\"advanced\" selected>Advanced</option>\n",
        "                </select>\n",
        "            </div>\n",
        "            <div class=\"mb-3 form-check\">\n",
        "                <input type=\"checkbox\" class=\"form-check-input\" id=\"enableCache\" checked>\n",
        "                <label class=\"form-check-label\" for=\"enableCache\">Enable Caching</label>\n",
        "            </div>\n",
        "            <button class=\"btn btn-primary\" id=\"setupBtn\">Save Settings</button>\n",
        "        </div>\n",
        "\n",
        "        <!-- Research Form -->\n",
        "        <div class=\"card mb-4\">\n",
        "            <div class=\"card-body\">\n",
        "                <form id=\"researchForm\">\n",
        "                    <div class=\"mb-3\">\n",
        "                        <label for=\"query\" class=\"form-label\">What would you like to research?</label>\n",
        "                        <input type=\"text\" class=\"form-control\" id=\"query\" placeholder=\"Enter your research query\">\n",
        "                    </div>\n",
        "                    <button type=\"submit\" class=\"btn btn-success\" id=\"researchBtn\">\n",
        "                        <i class=\"bi bi-search\"></i> Research\n",
        "                    </button>\n",
        "                    <div class=\"spinner-border text-primary loading\" id=\"loading\" role=\"status\">\n",
        "                        <span class=\"visually-hidden\">Loading...</span>\n",
        "                    </div>\n",
        "                </form>\n",
        "            </div>\n",
        "        </div>\n",
        "\n",
        "        <!-- Results Section -->\n",
        "        <div class=\"card mb-4\" id=\"resultsCard\" style=\"display: none;\">\n",
        "            <div class=\"card-header\">\n",
        "                <ul class=\"nav nav-tabs card-header-tabs\" id=\"resultTabs\">\n",
        "                    <li class=\"nav-item\">\n",
        "                        <a class=\"nav-link active\" data-bs-toggle=\"tab\" href=\"#answerTab\">Answer</a>\n",
        "                    </li>\n",
        "                    <li class=\"nav-item\">\n",
        "                        <a class=\"nav-link\" data-bs-toggle=\"tab\" href=\"#summaryTab\">Research Summary</a>\n",
        "                    </li>\n",
        "                    <li class=\"nav-item\">\n",
        "                        <a class=\"nav-link\" data-bs-toggle=\"tab\" href=\"#sourcesTab\">Sources</a>\n",
        "                    </li>\n",
        "                    <li class=\"nav-item\">\n",
        "                        <a class=\"nav-link\" data-bs-toggle=\"tab\" href=\"#processTab\">Process</a>\n",
        "                    </li>\n",
        "                </ul>\n",
        "            </div>\n",
        "            <div class=\"card-body\">\n",
        "                <div class=\"tab-content\">\n",
        "                    <div class=\"tab-pane fade show active\" id=\"answerTab\">\n",
        "                        <h4 id=\"queryDisplay\"></h4>\n",
        "                        <div id=\"finalAnswer\" class=\"mt-3\"></div>\n",
        "                    </div>\n",
        "                    <div class=\"tab-pane fade\" id=\"summaryTab\">\n",
        "                        <div id=\"researchSummary\"></div>\n",
        "                    </div>\n",
        "                    <div class=\"tab-pane fade\" id=\"sourcesTab\">\n",
        "                        <div id=\"searchResults\" class=\"list-group\"></div>\n",
        "                    </div>\n",
        "                    <div class=\"tab-pane fade\" id=\"processTab\">\n",
        "                        <div class=\"mb-3\">\n",
        "                            <h5>Original Query</h5>\n",
        "                            <p id=\"originalQuery\"></p>\n",
        "                        </div>\n",
        "                        <div class=\"mb-3\">\n",
        "                            <h5>Refined Query</h5>\n",
        "                            <p id=\"refinedQuery\"></p>\n",
        "                        </div>\n",
        "                        <div class=\"mb-3\">\n",
        "                            <h5>Draft Answer</h5>\n",
        "                            <div id=\"draftAnswer\"></div>\n",
        "                        </div>\n",
        "                        <div class=\"mb-3\">\n",
        "                            <h5>Metadata</h5>\n",
        "                            <pre id=\"metadata\" class=\"bg-light p-3 rounded\"></pre>\n",
        "                        </div>\n",
        "                    </div>\n",
        "                </div>\n",
        "            </div>\n",
        "        </div>\n",
        "\n",
        "        <!-- Search History -->\n",
        "        <div class=\"card\" id=\"historyCard\">\n",
        "            <div class=\"card-header bg-light\">\n",
        "                <h5>Search History</h5>\n",
        "            </div>\n",
        "            <div class=\"card-body\">\n",
        "                <ul class=\"list-group\" id=\"historyList\">\n",
        "                    <!-- History items will be added here -->\n",
        "                </ul>\n",
        "                <div class=\"text-center mt-3\" id=\"noHistory\">\n",
        "                    <p class=\"text-muted\">No search history yet</p>\n",
        "                </div>\n",
        "            </div>\n",
        "        </div>\n",
        "    </div>\n",
        "\n",
        "    <script src=\"https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/js/bootstrap.bundle.min.js\"></script>\n",
        "    <script src=\"https://cdn.jsdelivr.net/npm/marked/marked.min.js\"></script>\n",
        "    <script>\n",
        "        // Initialize variables\n",
        "        let isConfigured = false;\n",
        "        let searchHistory = [];\n",
        "\n",
        "        // DOM Elements\n",
        "        const setupBtn = document.getElementById('setupBtn');\n",
        "        const researchForm = document.getElementById('researchForm');\n",
        "        const resultsCard = document.getElementById('resultsCard');\n",
        "        const apiKeySection = document.getElementById('apiKeySection');\n",
        "        const historyList = document.getElementById('historyList');\n",
        "        const noHistory = document.getElementById('noHistory');\n",
        "\n",
        "        // Check if API keys are already set\n",
        "        async function checkConfiguration() {\n",
        "            try {\n",
        "                const response = await fetch('/api/config');\n",
        "                if (response.ok) {\n",
        "                    const config = await response.json();\n",
        "                    isConfigured = true;\n",
        "                    updateConfigUI(config);\n",
        "                    loadSearchHistory();\n",
        "                }\n",
        "            } catch (error) {\n",
        "                console.error('Error checking configuration:', error);\n",
        "            }\n",
        "        }\n",
        "\n",
        "        // Update the configuration UI\n",
        "        function updateConfigUI(config) {\n",
        "            document.getElementById('modelSelect').value = config.mistral_model;\n",
        "            document.getElementById('maxResults').value = config.max_search_results;\n",
        "            document.getElementById('temperature').value = config.temperature;\n",
        "            document.getElementById('temperatureValue').textContent = config.temperature;\n",
        "            document.getElementById('searchDepth').value = config.search_depth;\n",
        "            document.getElementById('enableCache').checked = config.enable_cache;\n",
        "        }\n",
        "\n",
        "        // Load search history\n",
        "        async function loadSearchHistory() {\n",
        "            try {\n",
        "                const response = await fetch('/api/history');\n",
        "                if (response.ok) {\n",
        "                    searchHistory = await response.json();\n",
        "                    renderSearchHistory();\n",
        "                }\n",
        "            } catch (error) {\n",
        "                console.error('Error loading search history:', error);\n",
        "            }\n",
        "        }\n",
        "\n",
        "        // Render search history\n",
        "        function renderSearchHistory() {\n",
        "            historyList.innerHTML = '';\n",
        "\n",
        "            if (searchHistory.length === 0) {\n",
        "                noHistory.style.display = 'block';\n",
        "                return;\n",
        "            }\n",
        "\n",
        "            noHistory.style.display = 'none';\n",
        "\n",
        "            searchHistory.forEach((item, index) => {\n",
        "                const li = document.createElement('li');\n",
        "                li.className = 'list-group-item history-item';\n",
        "                li.innerHTML = `\n",
        "                    <div class=\"d-flex justify-content-between align-items-center\">\n",
        "                        <span>${item.query}</span>\n",
        "                        <span class=\"text-muted small\">${new Date(item.timestamp).toLocaleString()}</span>\n",
        "                    </div>\n",
        "                `;\n",
        "                li.addEventListener('click', () => loadHistoryItem(index));\n",
        "                historyList.appendChild(li);\n",
        "            });\n",
        "        }\n",
        "\n",
        "        // Load a history item\n",
        "        function loadHistoryItem(index) {\n",
        "            const item = searchHistory[index];\n",
        "            displayResults(item);\n",
        "        }\n",
        "\n",
        "        // Setup API keys\n",
        "        setupBtn.addEventListener('click', async () => {\n",
        "            const tavilyApiKey = document.getElementById('tavilyApiKey').value;\n",
        "            const mistralApiKey = document.getElementById('mistralApiKey').value;\n",
        "            const model = document.getElementById('modelSelect').value;\n",
        "            const maxResults = parseInt(document.getElementById('maxResults').value);\n",
        "            const temperature = parseFloat(document.getElementById('temperature').value);\n",
        "            const searchDepth = document.getElementById('searchDepth').value;\n",
        "            const enableCache = document.getElementById('enableCache').checked;\n",
        "\n",
        "            if (!tavilyApiKey || !mistralApiKey) {\n",
        "                alert('Please enter both API keys');\n",
        "                return;\n",
        "            }\n",
        "\n",
        "            try {\n",
        "                const response = await fetch('/api/setup', {\n",
        "                    method: 'POST',\n",
        "                    headers: {\n",
        "                        'Content-Type': 'application/json'\n",
        "                    },\n",
        "                    body: JSON.stringify({\n",
        "                        tavily_api_key: tavilyApiKey,\n",
        "                        mistral_api_key: mistralApiKey,\n",
        "                        model,\n",
        "                        max_results: maxResults,\n",
        "                        temperature,\n",
        "                        search_depth: searchDepth,\n",
        "                        enable_cache: enableCache\n",
        "                    })\n",
        "                });\n",
        "\n",
        "                if (response.ok) {\n",
        "                    isConfigured = true;\n",
        "                    alert('Configuration saved successfully!');\n",
        "                } else {\n",
        "                    const error = await response.json();\n",
        "                    alert(`Error: ${error.detail || 'Failed to save configuration'}`);\n",
        "                }\n",
        "            } catch (error) {\n",
        "                console.error('Error setting up API keys:', error);\n",
        "                alert('Failed to save configuration. Please try again.');\n",
        "            }\n",
        "        });\n",
        "\n",
        "        // Update temperature value display\n",
        "        document.getElementById('temperature').addEventListener('input', (e) => {\n",
        "            document.getElementById('temperatureValue').textContent = e.target.value;\n",
        "        });\n",
        "\n",
        "        // Submit research form\n",
        "        researchForm.addEventListener('submit', async (e) => {\n",
        "            e.preventDefault();\n",
        "\n",
        "            if (!isConfigured) {\n",
        "                alert('Please set up API keys first');\n",
        "                return;\n",
        "            }\n",
        "\n",
        "            const query = document.getElementById('query').value.trim();\n",
        "            if (!query) {\n",
        "                alert('Please enter a research query');\n",
        "                return;\n",
        "            }\n",
        "\n",
        "            document.getElementById('researchBtn').style.display = 'none';\n",
        "            document.getElementById('loading').style.display = 'inline-block';\n",
        "\n",
        "            try {\n",
        "                const response = await fetch('/api/research', {\n",
        "                    method: 'POST',\n",
        "                    headers: {\n",
        "                        'Content-Type': 'application/json'\n",
        "                    },\n",
        "                    body: JSON.stringify({\n",
        "                        query,\n",
        "                        model: document.getElementById('modelSelect').value,\n",
        "                        max_results: parseInt(document.getElementById('maxResults').value),\n",
        "                        temperature: parseFloat(document.getElementById('temperature').value),\n",
        "                        search_depth: document.getElementById('searchDepth').value,\n",
        "                        enable_cache: document.getElementById('enableCache').checked\n",
        "                    })\n",
        "                });\n",
        "\n",
        "                if (response.ok) {\n",
        "                    const result = await response.json();\n",
        "                    displayResults(result);\n",
        "                    // Add to history if not already there\n",
        "                    if (!searchHistory.some(item => item.query === result.query)) {\n",
        "                        searchHistory.unshift(result);\n",
        "                        renderSearchHistory();\n",
        "                    }\n",
        "                } else {\n",
        "                    const error = await response.json();\n",
        "                    alert(`Error: ${error.detail || 'Research failed'}`);\n",
        "                }\n",
        "            } catch (error) {\n",
        "                console.error('Error performing research:', error);\n",
        "                alert('Research failed. Please try again.');\n",
        "            } finally {\n",
        "                document.getElementById('researchBtn').style.display = 'inline-block';\n",
        "                document.getElementById('loading').style.display = 'none';\n",
        "            }\n",
        "        });\n",
        "\n",
        "        // Display research results\n",
        "        function displayResults(result) {\n",
        "            // Show results card\n",
        "            resultsCard.style.display = 'block';\n",
        "\n",
        "            // Display query\n",
        "            document.getElementById('queryDisplay').textContent = result.query;\n",
        "            document.getElementById('originalQuery').textContent = result.query;\n",
        "            document.getElementById('refinedQuery').textContent = result.refined_query || 'No refinement needed';\n",
        "\n",
        "            // Display answers using Markdown\n",
        "            document.getElementById('finalAnswer').innerHTML = marked.parse(result.final_answer);\n",
        "            document.getElementById('draftAnswer').innerHTML = marked.parse(result.draft_answer);\n",
        "            document.getElementById('researchSummary').innerHTML = marked.parse(result.research_summary);\n",
        "\n",
        "            // Display metadata\n",
        "            document.getElementById('metadata').textContent = JSON.stringify(result.metadata, null, 2);\n",
        "\n",
        "            // Display search results\n",
        "            const searchResultsContainer = document.getElementById('searchResults');\n",
        "            searchResultsContainer.innerHTML = '';\n",
        "\n",
        "            if (result.research_results && result.research_results.length > 0) {\n",
        "                result.research_results.forEach((source, idx) => {\n",
        "                    const item = document.createElement('div');\n",
        "                    item.className = 'list-group-item';\n",
        "                    item.innerHTML = `\n",
        "                        <h5>${source.title || 'No title'}</h5>\n",
        "                        <p class=\"source-url\">\n",
        "                            <a href=\"${source.url}\" target=\"_blank\">${source.url}</a>\n",
        "                        </p>\n",
        "                        <p>${source.content ? source.content.substring(0, 200) + '...' : 'No content'}</p>\n",
        "                    `;\n",
        "                    searchResultsContainer.appendChild(item);\n",
        "                });\n",
        "            } else {\n",
        "                searchResultsContainer.innerHTML = '<div class=\"list-group-item\">No sources found</div>';\n",
        "            }\n",
        "\n",
        "            // Scroll to results\n",
        "            resultsCard.scrollIntoView({ behavior: 'smooth' });\n",
        "        }\n",
        "\n",
        "        // Check configuration on page load\n",
        "        checkConfiguration();\n",
        "    </script>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\")\n",
        "\n",
        "# Create static directory\n",
        "os.makedirs(\"static\", exist_ok=True)\n",
        "\n",
        "# Mount static files directory\n",
        "app.mount(\"/static\", StaticFiles(directory=\"static\"), name=\"static\")\n",
        "\n",
        "# Global configuration\n",
        "global_config = None\n",
        "\n",
        "# Get browser session\n",
        "def get_session(session_id: str = Cookie(None)):\n",
        "    if not session_id:\n",
        "        session_id = f\"session_{hashlib.md5(str(time.time()).encode()).hexdigest()}\"\n",
        "    if session_id not in session_store:\n",
        "        session_store[session_id] = {}\n",
        "    return session_id, session_store[session_id]\n",
        "\n",
        "# Setup endpoints\n",
        "@app.get(\"/\", response_class=HTMLResponse)\n",
        "async def get_home(request: Request):\n",
        "    logger.info(\"Loading home page\")\n",
        "    return templates.TemplateResponse(\"index.html\", {\"request\": request})\n",
        "\n",
        "@app.post(\"/api/setup\")\n",
        "async def setup(request: SetupRequest):\n",
        "    global global_config\n",
        "\n",
        "    try:\n",
        "        # Create configuration with API keys\n",
        "        config = AgentConfig(\n",
        "            tavily_api_key=request.tavily_api_key,\n",
        "            mistral_api_key=request.mistral_api_key,\n",
        "            mistral_model=request.model,\n",
        "            max_search_results=request.max_results,\n",
        "            temperature=request.temperature,\n",
        "            search_depth=request.search_depth,\n",
        "            enable_cache=request.enable_cache\n",
        "        )\n",
        "\n",
        "        # Store the configuration globally\n",
        "        global_config = config\n",
        "\n",
        "        logger.info(\"API keys and configuration saved successfully\")\n",
        "\n",
        "        return {\"status\": \"success\", \"message\": \"Configuration saved successfully\"}\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error saving API keys: {str(e)}\")\n",
        "        raise HTTPException(status_code=500, detail=f\"Failed to save configuration: {str(e)}\")\n",
        "\n",
        "@app.get(\"/api/config\", response_model=ConfigResponse)\n",
        "async def get_config():\n",
        "    if not global_config:\n",
        "        raise HTTPException(status_code=404, detail=\"Configuration not found\")\n",
        "\n",
        "    return {\n",
        "        \"mistral_model\": global_config.mistral_model,\n",
        "        \"max_search_results\": global_config.max_search_results,\n",
        "        \"temperature\": global_config.temperature,\n",
        "        \"search_depth\": global_config.search_depth,\n",
        "        \"enable_cache\": global_config.enable_cache,\n",
        "        \"cache_ttl\": global_config.cache_ttl,\n",
        "        \"max_retries\": global_config.max_retries,\n",
        "        \"verbose\": global_config.verbose\n",
        "    }\n",
        "\n",
        "# Search history storage - in production, use a database instead\n",
        "search_history = []\n",
        "\n",
        "@app.post(\"/api/research\", response_model=ResearchResponse)\n",
        "async def research_query(request: ResearchRequest):\n",
        "    global global_config\n",
        "\n",
        "    if not global_config:\n",
        "        raise HTTPException(status_code=400, detail=\"API keys not configured. Please setup first.\")\n",
        "\n",
        "    try:\n",
        "        # Create a copy of the global config\n",
        "        config = AgentConfig(\n",
        "            tavily_api_key=global_config.tavily_api_key,\n",
        "            mistral_api_key=global_config.mistral_api_key,\n",
        "            mistral_model=request.model or global_config.mistral_model,\n",
        "            max_search_results=request.max_results or global_config.max_search_results,\n",
        "            temperature=request.temperature or global_config.temperature,\n",
        "            search_depth=request.search_depth or global_config.search_depth,\n",
        "            enable_cache=request.enable_cache if request.enable_cache is not None else global_config.enable_cache\n",
        "        )\n",
        "\n",
        "        # Run the research\n",
        "        result = run_research_system(request.query, config)\n",
        "\n",
        "        # Format the response\n",
        "        response = {\n",
        "            \"query\": result[\"query\"],\n",
        "            \"refined_query\": result.get(\"refined_query\", \"\"),\n",
        "            \"research_summary\": result[\"research_summary\"],\n",
        "            \"draft_answer\": result[\"draft_answer\"],\n",
        "            \"final_answer\": result[\"final_answer\"],\n",
        "            \"research_results\": result.get(\"research_results\", []),\n",
        "            \"timestamp\": datetime.datetime.now().isoformat(),\n",
        "            \"metadata\": result.get(\"metadata\", {})\n",
        "        }\n",
        "\n",
        "        # Add to history\n",
        "        if len(search_history) >= 50:  # Limit history size\n",
        "            search_history.pop()\n",
        "        search_history.insert(0, response)\n",
        "\n",
        "        return response\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Research error: {str(e)}\")\n",
        "        raise HTTPException(status_code=500, detail=f\"Research failed: {str(e)}\")\n",
        "\n",
        "@app.get(\"/api/history\")\n",
        "async def get_history():\n",
        "    return search_history\n",
        "\n",
        "# Run the app using uvicorn# Modify the main block at the end of your script\n",
        "if __name__ == \"__main__\":\n",
        "    # Load API keys from environment if available\n",
        "    config = AgentConfig.from_env()\n",
        "    global_config = config\n",
        "\n",
        "    # Don't call uvicorn directly in Colab\n",
        "    logger.info(\"FastAPI app is configured and ready\")\n",
        "    print(\"To run this app, use the ngrok method below instead of the direct uvicorn call\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHPhEXI_0Oqr",
        "outputId": "5fad2517-86ce-42ab-d7f0-48f86852ef5f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To run this app, use the ngrok method below instead of the direct uvicorn call\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import nest_asyncio\n",
        "import uvicorn\n",
        "import threading\n",
        "from google.colab import output\n",
        "\n",
        "# Apply nest_asyncio to allow nested event loops\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Run the FastAPI app in a separate thread\n",
        "def run_app():\n",
        "    uvicorn.run(app, host=\"127.0.0.1\", port=8000)\n",
        "\n",
        "# Start the app in a thread\n",
        "thread = threading.Thread(target=run_app, daemon=True)\n",
        "thread.start()\n",
        "\n",
        "# Use Colab's port forwarding to expose the app\n",
        "output.serve_kernel_port_as_window(8000)\n",
        "\n",
        "print(\"FastAPI app is running. Click the URL above to access it.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "WiU_Jao61cZE",
        "outputId": "0f614c83-79d4-4fd4-d6bb-ce60b8f34fb1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mWarning: This function may stop working due to changes in browser security.\n",
            "Try `serve_kernel_port_as_iframe` instead. \u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, text, element) => {\n",
              "    if (!google.colab.kernel.accessAllowed) {\n",
              "      return;\n",
              "    }\n",
              "    element.appendChild(document.createTextNode(''));\n",
              "    const url = await google.colab.kernel.proxyPort(port);\n",
              "    const anchor = document.createElement('a');\n",
              "    anchor.href = new URL(path, url).toString();\n",
              "    anchor.target = '_blank';\n",
              "    anchor.setAttribute('data-href', url + path);\n",
              "    anchor.textContent = text;\n",
              "    element.appendChild(anchor);\n",
              "  })(8000, \"/\", \"https://localhost:8000/\", window.element)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FastAPI app is running. Click the URL above to access it.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [587]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "ERROR:    [Errno 98] error while attempting to bind on address ('127.0.0.1', 8000): address already in use\n",
            "INFO:     Waiting for application shutdown.\n"
          ]
        }
      ]
    }
  ]
}